### Appendix to 
# Universe and Mind 6: 
## Specificity, Generality, Causality-Control Units, Resolution of Causality-Control and Resolution of Perception, Matches etc.
#### 27.11.2023

[As of 27.11.2023: the main body of that work is not published yet]

Todor's Comments/Analysis of a comment, mapping to TOUM, 27.11.2023

From John's AI Group: https://discord.com/channels/1095629355432034344/1095695948807667782/1178263322206949396


````1)````
> johntcm — Днес в 11:17**
 _I always wonder why my goals can be very very specific, like I want to eat chocolate cake. Evolutionarily, we haven't had chocolate cake until like recently.
It could be any past state in memory can be a goal, and we keep updating this goal space as we learn new things about the world.
And we probably cannot have something we don't know about as a goal._

````2)```` 
>johntcm — Вчера в 9:39
>_you know I wonder whether every single intelligent task can be framed as a travelling salesman problem. Because then all you need is to plan and search_

**Todor/Tosh, 27.11.2023**
...
Hi, John, Josh, all.

````1) ````

Well, I don't wonder and the selection either of the actual specificity of the expression or realisation/implementation of a thought/decision/plan/prediction within a ladder of scales, and the one on which the evaluator focuses, depend namely on the evaluator-observer and her choice. "You", or more precisely the system of virtual causality-control units which could be extrapolated as having the causality-control of the body and mind at the very moment of producing the thought/the piece of knowledge/record/decision, can play around the scales and all "pieces" would sound correct in their POV. (That kind of agents/causality-control units (CCUs) always do what they believe was the best (sometimes it may seem random, because many policies could be "best", when there are no decisive options, and "all go"); when it seems that these CCUs/agents don't do what they predicted was best suggests that the evaluator has estimated a wrong agent's "identity" (the decisive model/forces which actually do cause the changes, behaviour, data, output with a given resolution of causality and/or perception), or they did wrong factorisation of the agent's motivation, planning horizon, goals etc.; etc.

See "Analysis of the meaning of a sentence based on the knowledge base of an operational thinking machine (...)", 2004 from TOUM (see below) and the example about deciding whether to eat a piece of chocolate *now* (in order to maximize the expected reward/pleasure of some set of causality-control units for the next few seconds) or not to eat - in order to maximize expected reward for the next months of other CCU, which expected/predicted, that eating chocolate would cause dental cavities and pain, remembering previous experience. Both are correct predictions within their models and their resolution, both are right and both are optimal: for the virtual CCUs for the very moment of making the decision, and for the long-term model/virtual-conditional CCU that would be true for the predicted horizon as well, as it may attribute not having a cavity for not eating chocolate, and there would be a confirmation.

Thus thre is not a single specific policy that is strictly globally definitely "optimal" for such minds/agents, because there is no single "objective" optimal goal or reward function for multiscale (in time and space), multilayer, multimodality, multidomain, multiview, multirange, multiagent, ... highly preemptive cognitive systems, which can change the whole-body goals and predictive horizon, main modality, domains etc. at every moment, often apparently randomly, based on some thoughts' flows running in the mind. "Everything goes" as long as the agent is still existing and the chosen CCU  provide high enough match and they end up "satisfied". 

There is no single unified stable objective self, it is extrapolated by the evaluator as an expected/predicted "integral" over supposed set of smaller local "selfs" ruling in different moments. See also: **Nature or Nurture: Socialization, Social Pressure, Reinforcement Learning, Reward Systems: Current Virtual Self - No Intrinsic Integral Self, but an Integral of Infinitesimal Local Selfs - Irrational Intentional Actions Are Impossible- Akrasia is Confused - Hypothesis about Socialization and Eye-Contact as an Oxytocin Source**: https://artificial-mind.blogspot.com/2012/11/nature-or-nurture-socialization-social.html

Every goal, as every prediction, or every record, has to be as precise as it is required in order to be possible to match it with others and to reach to a definite decision at some selected final step/stage/level. As one of the core operations and goals of the universe and mind is prediction-causation, which is matching, finding/discovering or/and causing a repetition, at one level or another the system must reach to a complete match and that often goes by reducing the resolution of causality-control and perception (probably that reminds you of FEP/AIF*, however mine is defined and published more than 20 years ago). The same is done in ML/DL, the filtering/convolutions for classification gradually reduce the resolution of the comparands in order to produce definite matches in the end; if it is required, there  could be an "out-of-the-neural-architecture" step for a categorical limit decision such as >0.5 or whatever boundary, e.g. if we want to pick YOLO's detection windows which have probability above a given threshold.

In one POV the "cake" exmple in particular is specific, because at the level of resolution of causality-control and perception you've chosen in the utterance. You have emphasized that you was searching namely for the aspect of "specificity" (you confirm that aspect by saying affirmative that "can be very very (twice emphasize) specific"), while it actually encompasses a lot of "generality" and lack of specificity as well, being encoded in a few words in natural language. That lack of specificy, the gaps, are completed by the actual sensory-motor context within the lower level virtual universes (the "reality" or a simulated world) and by the lower levels --, or maybe some are not precisely "lower", but *more specific*, aware of more specificity, mapping/addressing higher to lower  -- of the causality-control units within the chain of "command" in the system which is evaluated-observed-measured as a whole by another (or self-...) CCU (CCU, agent, mind, virtual universe, submachine - different terms from the Theory of Universe and Mind). 

For a simple illustration of the chain of command, a cascade, see for example the slides from the 2009 lecture at Technical University Sofia: "The Time Machine Exists: The Mind/Reason", or "The Principles of Intelligence: Ingelligence ~ Universe".

Generality/specificy in general settings is ambiguous and is selected and depends on the choice, capacities, focus/attention, desire, goals, filters, ... of the evaluator-observer and of some ladder and a system that's defined and that can clearly distinguish the cases.

At that, selected linguistic resolution, it is specific down to some resolution, or it is specific enough *for you*, in your mind. But even at a linguistic level it could be all kinds of "chocolate cake" - what brand or  homemade - how exactly it was made, what exact recipe, how much sugar or glucose-fructose syrop, milk or not, is it for vegans?, butter or plant-based oil, what exactly, what topping, is it or do you want it served cold or warm or hot, does it have fruits as well - which kinds, how much of them? How exactly it looks, is it a disk or a piece - how big, what exact color and how exactly it looks (chocolate, but it is darker, lighter, the locations of the parts, the texture; the shape down to so-and-so resolution, visual, spatial, color-luminance...?). I guess you probably don't care that much of these details, you like some of the *essential* features of the chocolate cakes, but thein if one says "I don't care" about these specifics, that means it was a general definition, i.e. not specific enough - for the lower levels of representation.

For this see the 2012 work in AGI, a collection of letters on the AGI email list and one additional article. "Chairs, Buildings, Caricatures and Optical Illusions as  ...": 

https://artificial-mind.blogspot.com/2021/01/capsnet-we-can-do-it-with-3d-point-clouds.html.html

https://research.twenkid.com/agi/2012/AGI_2012_Chairs_Caricatures_and_Object_Recognition_as_3D_Reconstruction.pdf

The high matches and predictions at linguistic level are explained for example in the intro points of "Universe and Mind 4", 2004 in an example which was repeated as a message by Y.Bengio in his 2017-2018 "Consciousness prior" line of thought. At higher levels of causality-control & perception humans do predict == control-cause "exactly" whatever they want with the highest possible resolution of CC in the virtual universes in these specific representations (linguistic), because intelligence/mind/CCU aim at achieving the highest match, predict with P=1 and to make no mistakes (or to "to minimize the prediction error" or the "free energy"); also the CCU want to *believe* and assume that they do cause-predict with the highest possible resolution of causality control within their controlled virtual universes. (In FEP/AIF context, I'd remark that the assumption that the internal states are conditionally independent from the external states is a *believe* of the CCU, or the "Markov Blanket", at its resolutions of CCU (or precision in their terms). Usually it is a simplification and is not strictly correct at the *mother universe's* resolution of CCU and in fact *all states* are "internal" for the lowest level encompassing virtual universe, at its "machine code", lowest level representation. That is explained in: 

**"The matrix in “The Matrix” is a matrix in the matrix"**, 4/2003, reprinted in English 20 years later: https://medium.com/@todorarnaudov/the-matrix-in-the-matrix-is-a-matrix-in-the-matrix-895e86c5f002 

You can't escape the *real/true/ultimate* matrix and all elements or maybe "states" as well are also/could be sensory on another lower level virtual universe, or they do have sensory component (can be influenced by) which is "under the radar" and the power of the current CCU. In the biological bodies there could be all kinds of electromagnetic, gravitational, poisonous (or any chemical substrates) forces which are not always detected by the typical "sensors", they can "hack" and modify the substrate, the code, on a lower level virtual universe than the one which is under control of the particular CCU, and the control is always at some precision which is lower than of the mother universe which actually causes and generates the behavior of the subuniverse. The very molecular and metabolic states modify the biological processes in the neural system, not just the perceptions which come as neural spikes, impulses or whatever (and the spikes are also such in an abstract representation in a mind which require to sample a spatio-temporal area, sequence of states, abstract/select particular aspects/measurements etc.). Every building particle/element of the system at the lowest level representation/encoding is/can be represented as a causality-control unit having both sensory and motor functions ("active" states)* and the lowest resolution models introduce errors.*

...

A continuation of the linguistic example - often or usually humans and CCUs do not realize, that the resolution of CCU&P of their output universe requires them to specify the *exact* state and trajectories of *every single measurable entity*, every particle, field, whatever, at the highest possible detail and frequency. Only if they do, they as CCU would have an actual, real, true "causality-control" force over that "subuniverse" (which is actually a mother universe, a top one). Humans don't, their prediction and casality-control are only virtual with some limit of resolution of causality and control and thus an accumulating error and a prediction-causation-perception "drift", as is the case with any subunverse that is different to the whole Universe (by the definitions and terminology of the classical TOUM). Another example is the few bits of consciously controlled output by a human by the supposed Reason/mind, compared to all processes that have to be described in full detail in order these 50 or 20 bits per second to be outputted, or in order to choose something that's 1 bit. All other "bits" are supposed to do more of the job as of their sheer number and influence, not the "free-willing" human which doesn't know what it is.

* Another analysis:

This is an example of mixing different levels of generality, aspects, POV.
Chocolate cake is recent, but having a goal to eat something so specific is not novel, the simplest living beings have food preferences. And if a monkey or herbivore is "on site", she probably wants and aims to pick "this" very berry or fruit or grass at this very location in front of her. Etc.
The predators want to eat "this" kind or this instance of an animal that they located and attacked and chased etc.



````2) ````
>johntcm — Вчера в 9:39
>_you know I wonder whether every single intelligent task can be framed as a travelling salesman problem. Because then all you need is to plan and search_

It depends what is to define and what is a task ("solving problems"), but generally said: yes, as to plan is to predict, and search (exploration) is required and it is a part of the most basic sensory-motor operations, starting from scanning and addressing single memory cells, and it is also about connectivity, continuity and discontinuity, other basic concepts of general intelligence, according to TOUM. (More on that will be published in the future).

To do: See/include comments at MLST Discord by Todor's (Tosh) summary of Connor Leahy's episode/debate about what intelligence was, his claims that it was "solving problems" and "prediction" - which IMO is correct, however that was not new, it was known "forever" (as of problems and goals) and the AGI pioneers of the early 2000s, for example myself T.A., defined and expressed very definitely and it was proven by the practical results in ML. "Problems/tasks/goals/plannings" are about prediction, match and search per se.

To do: See/include the comments on John's video about the memory-soup etc.

## Notes 

* Chris Fields has thoughts on the minimal agency etc. I have touched his research mainly from videos of discussions in Michael Levin's channel, with M.L. and their peers. It is related work to the topics of TOUM.

https://chrisfieldsresearch.com/

https://philpapers.org/rec/FIEUAM

* The classical works from the Theory of Universe and Mind were published between late 2001 - early 2004, with a pre-work about Seed AI in April 2001. The core theory is preceding some similar theories about general intelligence/AGI by Jeff Hawkins, Karl Friston and his school of thought, many of Levin's ideas and reasoning, ML ideas like Y.Bengio's "Consciousness Prior" or Lecun's "Path Towards Autonomous Machine Intelligence" and the claims of many others who restate or rediscover the principles and the directions drawn in TOUM. See the main page and the upcoming book "The Prophets of the Thinking Machines: AGI & Transhumanism: History, Theory and Pioneers".

* _A work in progress note_ : External and internal "state" also sometimes sound a bit confused concept in FEP/AIF for now; properties/state of the universe located with "external" *coordinates* maybe sound better. "Internal" as of TOUM are ones which are by default supposed to be more "causable" (controllable)-predictable than "external", such as muscles/effectors/actuators are those parts of the CCU/representations which are discovered as most predictable, some internal subsystems on the train of the operation as well, probably; in comparison to the sensory which by default are less predictable and less controllable and the mind aims at incrementing their controllability and turn the environmental external "states" into internal "states" and "territories". However that's not strict/requires elaboration as a mind may be unable to introspect itself or understand and control its mood swings etc. and there are "independent kingdoms" inside each CCU, which keeps its structure. The "environment" is dynamic and for the CCU/virtual universe that's the lowest level virtual universe to which the current one has access to influence it (to write) and from which it is infulenced and it receives sensory data (the influence could be not just "sensory" as from the _cognitive_ sensory matrices;  by default there are lower levels of representations of which it is not aware of or it has not control of, as with "The Matrix" article, and as with a computer which doesn't have access to the electrical parameters of its transistors from the level of machine code/computer architecture. I need to focus and elaborate on that and be more specific.

* Note, 2.4.2024: Also, the physical/geographical location may be irrelevant and not consistent – it could be a case where an agent can control better something which is physically more distant than something else and not connecte by the expected biological way. E.g. brain implants or even ordinary remote controls. Depending on the measurement, one can have more “conscious-will-control” over a space probe than to his own metabolism for example. For his mind at a high level that remote-controlled probe/robot, say on the Moon, is more tightly coupled to his sensory-motor chains than his “own” guts. (That’s also maybe to suggest and illustrate another observation from TOUM, that “guts control us”, more technically – the lower level processes/virtual universes control the higher level ones, in the constructive hierarchy. Some may say the opposite, in FEP/AIF school the higher level limit the possibilities of the lower levels etc., but it is also the reverse: the lower level allow the very existence of the higher levels and when a higher level “dies”, the lower representations do not die, ultimately there’s always a lowest level, which in TOUM is the “physics”, where  “everything goes”, every possible state is well defined and it goes “forever” from the POV of any higher level CCU/virtual universe)


